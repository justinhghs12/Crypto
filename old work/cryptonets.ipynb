{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"seaborn\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from binance.client import Client\n",
    "import time\n",
    "import statistics as stats\n",
    "from datetime import datetime\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlfinlab\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "\n",
    "directory = '../data/final_data'\n",
    "all_files = glob.glob(directory + \"/*.csv\")\n",
    "\n",
    "for file in all_files:\n",
    "    \n",
    "    df = pd.read_csv(file, index_col=None, header=0).drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    df_dict[file.split('\\\\')[1][:-10]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open Time</th>\n",
       "      <th>Time of Day</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close Time</th>\n",
       "      <th>Quote asset vol</th>\n",
       "      <th>Num trades</th>\n",
       "      <th>...</th>\n",
       "      <th>fastd</th>\n",
       "      <th>SAR</th>\n",
       "      <th>ULTOSC</th>\n",
       "      <th>WILLR</th>\n",
       "      <th>AD</th>\n",
       "      <th>ADOSC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>ATR</th>\n",
       "      <th>NATR</th>\n",
       "      <th>Spinning Top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-04 03:00:00.000</td>\n",
       "      <td>10800</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1991422.0</td>\n",
       "      <td>2017-12-04 03:29:59.999</td>\n",
       "      <td>22.743129</td>\n",
       "      <td>220.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.932193</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>31.202095</td>\n",
       "      <td>-82.432432</td>\n",
       "      <td>-1.115105e+07</td>\n",
       "      <td>-2.522361e+06</td>\n",
       "      <td>45800177.0</td>\n",
       "      <td>1.994172e-07</td>\n",
       "      <td>1.994172e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-04 03:30:00.000</td>\n",
       "      <td>12600</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4123602.0</td>\n",
       "      <td>2017-12-04 03:59:59.999</td>\n",
       "      <td>46.809428</td>\n",
       "      <td>316.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.017794</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>29.874065</td>\n",
       "      <td>-98.780488</td>\n",
       "      <td>-1.494476e+07</td>\n",
       "      <td>-3.558265e+06</td>\n",
       "      <td>41676575.0</td>\n",
       "      <td>2.030303e-07</td>\n",
       "      <td>2.030303e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04 04:00:00.000</td>\n",
       "      <td>14400</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2136313.0</td>\n",
       "      <td>2017-12-04 04:29:59.999</td>\n",
       "      <td>24.021883</td>\n",
       "      <td>221.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.684461</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>28.369373</td>\n",
       "      <td>-96.103896</td>\n",
       "      <td>-1.632708e+07</td>\n",
       "      <td>-4.098395e+06</td>\n",
       "      <td>39540262.0</td>\n",
       "      <td>2.006710e-07</td>\n",
       "      <td>2.006710e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-04 04:30:00.000</td>\n",
       "      <td>16200</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2828688.0</td>\n",
       "      <td>2017-12-04 04:59:59.999</td>\n",
       "      <td>31.831697</td>\n",
       "      <td>257.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.387506</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>29.840368</td>\n",
       "      <td>-88.888889</td>\n",
       "      <td>-1.745855e+07</td>\n",
       "      <td>-4.306790e+06</td>\n",
       "      <td>42368950.0</td>\n",
       "      <td>2.006230e-07</td>\n",
       "      <td>2.006230e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-04 05:00:00.000</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1820903.0</td>\n",
       "      <td>2017-12-04 05:29:59.999</td>\n",
       "      <td>20.481454</td>\n",
       "      <td>218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.117726</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>35.496836</td>\n",
       "      <td>-80.555556</td>\n",
       "      <td>-1.715507e+07</td>\n",
       "      <td>-3.903954e+06</td>\n",
       "      <td>44189853.0</td>\n",
       "      <td>2.034357e-07</td>\n",
       "      <td>2.034357e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open Time  Time of Day      Open      High       Low  \\\n",
       "0  2017-12-04 03:00:00.000        10800  0.000011  0.000012  0.000011   \n",
       "1  2017-12-04 03:30:00.000        12600  0.000011  0.000011  0.000011   \n",
       "2  2017-12-04 04:00:00.000        14400  0.000011  0.000011  0.000011   \n",
       "3  2017-12-04 04:30:00.000        16200  0.000011  0.000011  0.000011   \n",
       "4  2017-12-04 05:00:00.000        18000  0.000011  0.000011  0.000011   \n",
       "\n",
       "      Close     Volume               Close Time  Quote asset vol  Num trades  \\\n",
       "0  0.000011  1991422.0  2017-12-04 03:29:59.999        22.743129       220.0   \n",
       "1  0.000011  4123602.0  2017-12-04 03:59:59.999        46.809428       316.0   \n",
       "2  0.000011  2136313.0  2017-12-04 04:29:59.999        24.021883       221.0   \n",
       "3  0.000011  2828688.0  2017-12-04 04:59:59.999        31.831697       257.0   \n",
       "4  0.000011  1820903.0  2017-12-04 05:29:59.999        20.481454       218.0   \n",
       "\n",
       "   ...      fastd       SAR     ULTOSC      WILLR            AD         ADOSC  \\\n",
       "0  ...  65.932193  0.000009  31.202095 -82.432432 -1.115105e+07 -2.522361e+06   \n",
       "1  ...  56.017794  0.000009  29.874065 -98.780488 -1.494476e+07 -3.558265e+06   \n",
       "2  ...  22.684461  0.000009  28.369373 -96.103896 -1.632708e+07 -4.098395e+06   \n",
       "3  ...   9.387506  0.000009  29.840368 -88.888889 -1.745855e+07 -4.306790e+06   \n",
       "4  ...  31.117726  0.000009  35.496836 -80.555556 -1.715507e+07 -3.903954e+06   \n",
       "\n",
       "          OBV           ATR          NATR  Spinning Top  \n",
       "0  45800177.0  1.994172e-07  1.994172e-07           0.0  \n",
       "1  41676575.0  2.030303e-07  2.030303e-07           0.0  \n",
       "2  39540262.0  2.006710e-07  2.006710e-07           1.0  \n",
       "3  42368950.0  2.006230e-07  2.006230e-07           0.0  \n",
       "4  44189853.0  2.034357e-07  2.034357e-07           1.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['ADA'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_list = list(df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Close_pct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df_dict[coin_list[0]].columns)\n",
    "not_quant = ['Open Time','Close Time']\n",
    "quants = [i for i in columns if i not in not_quant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADA', 'BNB', 'ETH', 'LINK', 'LTC', 'XRP']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process():\n",
    "    def __init__(self, data, test_size=.1, seq_len=25):\n",
    "        self.seq_len = seq_len       \n",
    "        self.test_size = test_size\n",
    "        self.sequences = []\n",
    "        self.data = data\n",
    "        self.normalize_data = None\n",
    "        #self.reduce()\n",
    "        self.create_sequences()\n",
    "        \n",
    "    def reduce(self):\n",
    "        \n",
    "        #lower = self.data.shape[0] - int(.3*self.data.shape[0]) - 336\n",
    "        #upper = self.data.shape[0] - int(.3*self.data.shape[0]) + 336\n",
    "        \n",
    "        current = self.data.shape[0]\n",
    "        \n",
    "        while current%self.seq_len!=0:\n",
    "            current -= 1\n",
    "            \n",
    "        self.data = self.data[:current]\n",
    "        \n",
    "    def create_sequences(self):\n",
    "        \n",
    "        #indices = list(range(0, self.data.shape[0], self.seq_len))\n",
    "        \n",
    "        for i in range(self.data.shape[0] - self.seq_len):\n",
    "            self.sequences.append(torch.tensor(list(self.data[i:i+self.seq_len])))\n",
    "            \n",
    "        self.sequences = torch.stack(self.sequences)\n",
    "        self.sequences = self.sequences.reshape(self.sequences.shape[0], self.seq_len, 1)\n",
    "        \n",
    "        \n",
    "    def separate(self):\n",
    "        #split_off = int(self.sequences.shape[0] * self.test_size)\n",
    "        \n",
    "        lower = self.data.shape[0] - int(self.test_size*self.data.shape[0]) - int(.05*self.data.shape[0])\n",
    "        upper = self.data.shape[0] - int(self.test_size*self.data.shape[0]) + int(.05*self.data.shape[0])\n",
    "        \n",
    "        training = self.sequences[:lower]\n",
    "        testing = self.sequences[upper:]\n",
    "        \n",
    "        training_x, training_y = training[:, :-1, :], training[:, -1, :]\n",
    "        testing_x, testing_y = testing[:, :-1, :], testing[:, -1, :]\n",
    "        \n",
    "        training_data = (training_x, training_y)\n",
    "        testing_data = (testing_x, testing_y)\n",
    "        \n",
    "        return training_data, testing_data\n",
    "    \n",
    "    def normalize(self):\n",
    "        self.normalize_data = (self.data - stats.mean(self.data))/stats.stdev(self.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_files='E:/Users/Justin/Desktop/Course Folders 2020/practicum/tensors/25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA : \n",
      "torch.Size([51486, 25, 1])\n",
      "training:  torch.Size([43785, 24, 1]) torch.Size([43785, 1])\n",
      "testing:  torch.Size([2551, 24, 1]) torch.Size([2551, 1])\n",
      "\n",
      "\n",
      "BNB : \n",
      "torch.Size([58160, 25, 1])\n",
      "training:  torch.Size([49458, 24, 1]) torch.Size([49458, 1])\n",
      "testing:  torch.Size([2884, 24, 1]) torch.Size([2884, 1])\n",
      "\n",
      "\n",
      "ETH : \n",
      "torch.Size([58180, 25, 1])\n",
      "training:  torch.Size([49475, 24, 1]) torch.Size([49475, 1])\n",
      "testing:  torch.Size([2885, 24, 1]) torch.Size([2885, 1])\n",
      "\n",
      "\n",
      "LINK : \n",
      "torch.Size([54517, 25, 1])\n",
      "training:  torch.Size([46361, 24, 1]) torch.Size([46361, 1])\n",
      "testing:  torch.Size([2702, 24, 1]) torch.Size([2702, 1])\n",
      "\n",
      "\n",
      "LTC : \n",
      "torch.Size([58180, 25, 1])\n",
      "training:  torch.Size([49475, 24, 1]) torch.Size([49475, 1])\n",
      "testing:  torch.Size([2885, 24, 1]) torch.Size([2885, 1])\n",
      "\n",
      "\n",
      "XRP : \n",
      "torch.Size([52839, 25, 1])\n",
      "training:  torch.Size([44935, 24, 1]) torch.Size([44935, 1])\n",
      "testing:  torch.Size([2618, 24, 1]) torch.Size([2618, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li4={}\n",
    "li={}\n",
    "\n",
    "for coin in coin_list:\n",
    "    print(coin, ': ')\n",
    "    close = df_dict[coin]['Close']\n",
    "    process = Process(close)\n",
    "    print(process.sequences.shape)\n",
    "    li4[coin]=process.sequences\n",
    "    train, test = process.separate()\n",
    "    print('training: ', train[0].shape, train[1].shape)\n",
    "    print('testing: ', test[0].shape, test[1].shape)\n",
    "    print('\\n')\n",
    "    li[coin] = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Close_pct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li4={}\n",
    "li={}\n",
    "\n",
    "for coin in coin_list:\n",
    "    print(coin, ': ')\n",
    "    close = df_dict[coin]['Close_pct']\n",
    "    process = Process(close)\n",
    "    print(process.sequences.shape)\n",
    "    li4[coin]=process.sequences\n",
    "    train, test = process.separate()\n",
    "    print('training: ', train[0].shape, train[1].shape)\n",
    "    print('testing: ', test[0].shape, test[1].shape)\n",
    "    print('\\n')\n",
    "    li[coin] = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(li4, tensor_files+'/complete25.pth')\n",
    "torch.save(li, tensor_files+'/complete25train_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.fc2 = nn.Linear(300, 150)\n",
    "        self.fc3 = nn.Linear(150, 60)\n",
    "        self.fc4 = nn.Linear(60, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return self.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True).to(device)\n",
    "        self.l1 = nn.Linear(hidden_dim, output_dim).to(device)\n",
    "        #self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim).requires_grad_()\n",
    "        #h0=h0.to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim).requires_grad_()\n",
    "        #c0=c0.to(device)\n",
    "        output, (hn, cn) = self.lstm(x.to(device), (h0.detach().to(device), c0.detach().to(device)))\n",
    "        #out = self.relu(self.l1(output[:, -1, :]))\n",
    "        out = self.l1(output[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.l1 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim).requires_grad_()\n",
    "        output, (hn, cn) = self.gru(x, (h0.detach(), c0.detach())) \n",
    "        out = self.l1(output[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    \n",
    "    def __init__(self, model, loss, weights_path, coin, training_data, testing_data, epochs=50, batch_size=1):\n",
    "            \n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=.01)\n",
    "        self.criterion = loss\n",
    "        self.x_train, self.y_train = training_data\n",
    "        self.x_test, self.y_test = testing_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_list = []\n",
    "        self.train_time_list = []\n",
    "        self.test_time_list = []\n",
    "        self.results = []\n",
    "        self.coin=coin\n",
    "        self.weights_path=weights_path\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "        \n",
    "            start = time.time()\n",
    "            \n",
    "            #indices = random.sample(list(range(0, self.x_train.shape[0])), k=5_000)\n",
    "            indices = random.sample(list(range(0, self.x_train.shape[0])), k=10_000)\n",
    "\n",
    "            #for i in range(self.x_train.shape[0]):\n",
    "            for i in indices:\n",
    "                \n",
    "                if self.batch_size > 1:\n",
    "                    \n",
    "                    sequence = self.x_train[indices[indices.index(i)*self.batch_size]:indices[indices.index(i)*self.batch_size+self.batch_size]].to(device)\n",
    "                    label = self.x_train[indices[indices.index(i)*self.batch_size]:indices[indices.index(i)*self.batch_size+self.batch_size]].to(device)\n",
    "\n",
    "                else:\n",
    "                    sequence = self.x_train[i].reshape(self.batch_size, self.x_train.shape[1], 1).to(device)\n",
    "                    label = self.y_train[i].to(device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                #self.model.zero_grad()\n",
    "                result = self.model(sequence)\n",
    "                loss = self.criterion(result, label)\n",
    "                self.loss_list.append(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                                \n",
    "            end = time.time()\n",
    "            tot_time = end - start\n",
    "            \n",
    "            self.train_time_list.append(tot_time)\n",
    "            \n",
    "            if _%2==0 or _==self.epochs-1:\n",
    "\n",
    "                print('Epoch: ', _ + 1, 'Time: ', self.train_time_list[-1], 'Loss: ', sum(self.loss_list))\n",
    "            \n",
    "                saver=(self.epochs, self.optimizer, self.criterion, self.model.state_dict(), self.model)\n",
    "                \n",
    "                torch.save(saver, self.weights_path+self.coin+'params.pth')\n",
    "            \n",
    "            self.loss_list=[]\n",
    "                \n",
    "    def test(self):\n",
    "        start = time.time()\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(self.weights_path+self.coin+'params.pth')[3])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for i in range(self.x_test.shape[0]):\n",
    "\n",
    "                sequence = self.x_test[i].to(device)\n",
    "                #label = self.y_test[i]\n",
    "\n",
    "                result = self.model(sequence.reshape(self.batch_size, sequence.shape[0], 1))\n",
    "                self.results.append(result)\n",
    "\n",
    "        \n",
    "        end = time.time()\n",
    "        tot_time = end-start\n",
    "        self.test_time_list.append(tot_time)\n",
    "    \n",
    "    def min_max_scaling(self, x):\n",
    "        return (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41210, 24, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li['ADA'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path='E:/Users/Justin/Desktop/Course Folders 2020/practicum/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(x):\n",
    "    return (x-torch.min(x))/(torch.max(x)-torch.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 24\n",
    "n_layers = 5\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTM(input_dim, hidden_dim, n_layers, output_dim)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA\n",
      "Epoch:  1 Time:  152.57839488983154 Loss:  0.001727681139875188\n",
      "Epoch:  3 Time:  152.41341161727905 Loss:  0.0007587072779339167\n",
      "Epoch:  5 Time:  153.8522183895111 Loss:  0.0007568692625206127\n",
      "Epoch:  7 Time:  152.00651931762695 Loss:  0.0007523928864851607\n",
      "Epoch:  9 Time:  146.1497049331665 Loss:  0.0007499804718756512\n",
      "Epoch:  11 Time:  147.79075956344604 Loss:  0.0007576726573049542\n",
      "Epoch:  13 Time:  147.03528428077698 Loss:  0.0007516282426762449\n",
      "Epoch:  15 Time:  147.79920268058777 Loss:  0.0007521786323842474\n"
     ]
    }
   ],
   "source": [
    "coin='ADA'\n",
    "train, test=li[coin]\n",
    "print(coin)\n",
    "optimize = Optimization(model, criterion, weights_path, coin, train, test, epochs=15)\n",
    "optimize.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimize.test()\n",
    "\n",
    "plt.plot(li['ADA'][1][1], color='r')\n",
    "plt.plot(optimize.results, color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'testing_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-23e3562221ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mli\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoin\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0moptimize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'testing_data'"
     ]
    }
   ],
   "source": [
    "weights_path='E:/Users/Justin/Desktop/Course Folders 2020/practicum/weights/'\n",
    "\n",
    "for coin in coin_list:\n",
    "    \n",
    "    input_dim = 1\n",
    "    hidden_dim = 24\n",
    "    n_layers = 2\n",
    "    output_dim = 1\n",
    "\n",
    "    model = LSTM(input_dim, hidden_dim, n_layers, output_dim)\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "    \n",
    "    \n",
    "    train, test=li[coin]\n",
    "    print(coin)\n",
    "    optimize = Optimization(model, criterion, weights_path, coin, train, test, epochs=15)\n",
    "    optimize.train()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MSELoss' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e5f5c4c378da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mli\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ADA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-6d0e32b1480d>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoin\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'params.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Justin\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[1;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_metadata'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Justin\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 576\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MSELoss' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "coin='ADA'\n",
    "optimize = Optimization(optimizer, model, criterion, weights_path, coin, train, test, epochs=15)\n",
    "\n",
    "optimize.test()\n",
    "\n",
    "plt.plot(li['ADA'][1][1], color='r')\n",
    "plt.plot(optimize.results, color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li['ADA'][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(li['ADA'][1][1])\n",
    "plt.plot(optimize.results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minisgd\n",
    "#sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = 'BTC LINK VET BNB ETH XRP XTZ LTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(optimize.results)\n",
    "plt.plot(test[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(input_dim, hidden_dim, n_layers, output_dim)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=.01)\n",
    "\n",
    "optimize = Optimization(optimizer, model, criterion, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test[1])\n",
    "plt.plot(optimize.results)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
